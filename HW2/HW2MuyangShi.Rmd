---
title: "HW2MuyangShi"
author: "Muyang Shi"
date: "2024-02-08"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(Rcpp)
library(RcppArmadillo)
library(dplyr)
library(knitr)
library(gridExtra)
library(latex2exp)
library(ggplot2)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
sourceCpp('optimize.cpp')
```

Note: the cpp source code to this document can be found on my Github, listed as `optimize.cpp`, [\textcolor{blue}{here}](https://github.com/muyangshi/STAT600Homework/tree/main/HW2).

## Problem 1

### (a)

Using the density, we can derive that (with $n$ observations):
\begin{align*}
  l(\theta) &= -n \log \pi - \sum_{i=1}^n \log(1 + (x_i - \theta)^2) \\
  l'(\theta) &= \sum_{i=1}^n \dfrac{2(x_i - \theta)}{1 + (x_i - \theta)^2} \\
  l''(\theta) &= \sum_{i=1}^n \dfrac{-2 + 2(x_i - \theta)^2}{(1 + (x_i - \theta)^2)^2}
\end{align*}

Here is a plot of the first derivative of the log likelihood $l'(\theta)$: note that the vertical line is drawn where the derivative of the log likelihood equals zero, at $\hat{\theta} = 1.188$

```{r, echo = FALSE, out.width = '60%', fig.align='center'}
cauchy_data <- c(-8.86, -6.82, -4.03, -2.84, 0.14, 0.19, 0.24, 0.27, 0.49, 0.62, 0.76, 1.09,
                 1.18, 1.32, 1.36, 1.58, 1.58, 1.78, 2.13, 2.15, 2.36, 4.05, 4.11, 4.12, 6.83)
theta_grid <- seq(from = -10, to = 10, length.out = 401)
plot(theta_grid, sapply(theta_grid, dloglik_cauchy_cpp, x = cauchy_data), 
     type = 'l',main = 'Graph of Cauchy Score Function',
     xlab = 'theta', ylab = 'score')
abline(h = 0, col = 'blue', lty = 2)
abline(v = 1.187943)
```

### (b)

**i. Bisection**

```{r}
Bisection_theta_hat <- Bisection_cauchy_cpp(a=0,b=3,dat=cauchy_data, eps=1e-8)
```

**ii. Newton-Raphson**

```{r}
Newton_theta_hat <- Newton_cauchy_cpp(x = 0, dat=cauchy_data, eps=1e-8)
```

**iii. Fisher Scoring**

```{r}
Fisher_theta_hat <- FisherScoring_cauchy_cpp(theta = 0, dat=cauchy_data, eps = 1e-8)
```

**iv. Secant Method**

```{r}
Secant_theta_hat <- Secant_cauchy_cpp(0, 3, dat=cauchy_data, eps = 1e-8)
```

### (c)

```{r, echo = FALSE}
rbind(c("Bisection", round(Bisection_theta_hat,4), 28), 
      c("Newton Raphson",round(Newton_theta_hat,4), 6),
      c("Fisher Scoring",round(Fisher_theta_hat,4), 6), 
      c("Secant",round(Secant_theta_hat,4), 7)) %>%
  data.frame() %>%
  kable(col.names = c("Method", "theta_hat", "Iters to Converge"),
        caption = "Results of Estimation")
```

### (d)

I used the absolute convergence criteria with an $\epsilon = 1 \times 10^{-8}$, i.e. it mandates stopping when $$\left| \hat{\theta}^{t+1} - \hat{\theta}^t\right| < \epsilon$$

### (e)

```{r, results='hide'}
1/sqrt(-ddloglik_cauchy_cpp(Bisection_theta_hat, cauchy_data))
```

There is no "best" estimate of $\theta$, as the four methods produce the same the point estimates $\hat{\theta} = 1.188$. The standard error of the estimate can be calculated using the fisher information evaluated at the estimate, $$SE(\hat{\theta}) = \dfrac{1}{\sqrt{I(\hat{\theta})}} = \dfrac{1}{\sqrt{-l''(\hat{\theta})}} = 0.281$$

### (f)

From the visual examination (i.e. "eye-balling") of the plot of the score function, we see that it crosses zero once and only once somewhere between $\theta \in (0, 3)$. Therefore, 

- we initialized the Bisection solver with the two endpoints being 0 and 3. The result "should" not be sensitive to where we chose the two endpoints because the score function crosses zero only once, as long as that $\hat{\theta} = 1.188$ is within the search range between the two endpoints;

- for the other three Newton-like methods (Newton-Raphson, Fisher Scoring, and the Secant methods), calculation for the second derivative could potentially lead to trouble especially for the Newton-Raphson and Fisher Scoring methods. As illustrated in the example below, when we feed the algorithms and initial values (e.g. $\hat{\theta} = 3$ or $\hat{\theta} = -1$) that are near regions of $\l''(\hat{\theta}) = 0$, the algorithm will run into non-convergence as the second derivative is on the denominator, and when the denominator turns zero it causes trouble -- see the two `tryCatch` error below. As for the Secant method the above rationale stays the same as we are approximating the derivative each time only; this means that we can certainly run into the same issue and it would not converge. 

```{r,echo = FALSE,out.width = '60%', fig.align='center'}
plot(theta_grid, sapply(theta_grid, ddloglik_cauchy_cpp, x = cauchy_data), 
     type = 'l',main = 'Graph of Second Derivative',
     xlab = 'theta', ylab = "l''")
abline(h = 0)
abline(v = -1, col = 'blue', lty = 2)
abline(v = 3, col = 'blue', lty = 2)
```

```{r}
tryCatch(Newton_cauchy_cpp(x = 3, dat=cauchy_data, eps=1e-8), 
         error = print)
tryCatch(FisherScoring_cauchy_cpp(theta = -1, dat=cauchy_data, eps=1e-8), 
         error = print)
```

### (g)

Below is a graph of the new score function with the new data added:

```{r, echo = FALSE, out.width = '60%', fig.align = 'center'}
cauchy_data2 <- c(-8.34,-1.73,-0.40,-0.24, 0.60, 0.94, 1.05, 1.06, 1.45,1.50,
                  1.54, 1.72, 1.74, 1.88, 2.04, 2.16, 2.39, 3.01, 3.01,3.08,
                  4.66, 4.99, 6.01, 7.06,25.45)
cauchy_data_full <- c(cauchy_data, cauchy_data2)
plot(theta_grid, sapply(theta_grid, dloglik_cauchy_cpp, x = cauchy_data_full), 
     type = 'l',main = 'Graph of Cauchy Score Function',
     xlab = 'theta', ylab = 'score')
abline(h = 0, col = 'blue', lty = 2)
abline(v = 1.471299)
```

Using the four methods would actually still give the same estimates of $\hat{\theta}$ as long as we feed them the appropriate starting values:

```{r, echo = FALSE}
Bisection_theta_hat2 <- Bisection_cauchy_cpp(0,5,cauchy_data_full)
Newton_theta_hat2 <- Newton_cauchy_cpp(1, cauchy_data_full)
Fisher_theta_hat2 <- FisherScoring_cauchy_cpp(2, cauchy_data_full)
Secant_theta_hat2 <- Secant_cauchy_cpp(0, 3, cauchy_data_full)
rbind(c("Bisection", round(Bisection_theta_hat2,4), 28), 
      c("Newton Raphson",round(Newton_theta_hat2,4), 4),
      c("Fisher Scoring",round(Fisher_theta_hat2,4), 5), 
      c("Secant",round(Secant_theta_hat2,4), 6)) %>%
  data.frame() %>%
  kable(col.names = c("Method", "theta_hat", "Iters to Converge"),
        caption = "Results of Estimation")
```

Hence, our best estimate of $\hat{\theta}$ is 1.471, with a standard error of 0.197.

```{r}
Bisection_theta_hat2
1/sqrt(-ddloglik_cauchy_cpp(Bisection_theta_hat2, cauchy_data_full))
```

## Problem 2

From the course slides, we know that Newton's method has quadratic convergence order $\beta = 2$, i.e. $$\lim_{t\rightarrow \infty} \dfrac{|\epsilon^{(t+1)}|}{|\epsilon^{(t)}|^2} = c$$

As for the Secant method, from the textbook equation 2.27, we have that as $t \rightarrow \infty$
$$\epsilon^{(t+1)} \approx d^{(t)}\epsilon^{(t)}\epsilon^{(t-1)}$$, where $$d^{(t)} \rightarrow \dfrac{g'''(x^*)}{2g''(x^*)} = d$$
Next, to find the $\beta$ such that $$\lim_{t\rightarrow \infty}\dfrac{|\epsilon^{(t+1)}|}{|\epsilon^{(t)}|^\beta} = c$$
we use this relationship to replace $\epsilon^{(t-1)}$ and $\epsilon^{(t+1)}$ in the equation above, we will get as $t \rightarrow \infty$, $$c |\epsilon^{(t)}|^\beta = d |\epsilon^{(t)}| \dfrac{|\epsilon^{(t)}|}{c}^{1/\beta}$$ with rearrangement we have $$\lim_{t \rightarrow \infty} |\epsilon^{(t)}|^{1-\beta+1/\beta} = \dfrac{c^{1+1/\beta}}{d} = c^*$$ where $c^*$ is just some constant, i.e. $1-\beta + 1/\beta = 0$.
Finally, solving for $\beta$ yields $$\beta = (1 + \sqrt{5})/2 \approx 1.62 < 2$$.

Hence, the Newton's method enjoys a faster convergence rate than the Secant method.

## Problem 3

### (a)

Denote $\boldsymbol{X_i\beta} = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}$, we can write the likelihood for this problem as (treated as binomials):
\begin{align*}
L(\boldsymbol{\beta}; \boldsymbol{X}) &= \prod_{i=1}^n \left(\dfrac{\exp(\boldsymbol{X_i\beta})}{1+\exp(\boldsymbol{X_i\beta})}\right)^{y_i} \left(1-\dfrac{\exp(\boldsymbol{X_i\beta})}{1+\exp(\boldsymbol{X_i\beta})}\right)^{1-y_i} \\
&= \prod_{i=1}^n \left(\dfrac{\exp(\boldsymbol{X_i\beta})}{1+\exp(\boldsymbol{X_i\beta})}\right)^{y_i} \left(\dfrac{1}{1+\exp(\boldsymbol{X_i\beta})} \right)^{1-y_i} \\
&= \prod_{i=1}^n \dfrac{(\exp(\boldsymbol{X_i\beta}))^{y_i}}{1+\exp(\boldsymbol{X_i\beta})}
\end{align*}
Hence the log likelihood is:
\begin{align*}
l(\boldsymbol{\beta}; \boldsymbol{X}) &= \sum_{i=1}^n y_i * \log(\exp(\boldsymbol{X_i\beta})) - (1+\exp(\boldsymbol{X_i\beta})) \\
&= \sum_{i=1}^n y_i \boldsymbol{X_i\beta} - (1+\exp(\boldsymbol{X_i\beta}))
\end{align*}

### (b)

To use the Newton-Raphson method, we need the first and the second derivatives with respect to $\boldsymbol{\beta}$:
\begin{align*}
\boldsymbol{g'(\beta)} \equiv \dfrac{l(\boldsymbol{\beta}; \boldsymbol{X})}{\partial \boldsymbol{\beta}} &= \sum_{i=1}^n \left[y_i \dfrac{\partial}{\partial\boldsymbol{\beta}}\boldsymbol{X_i \beta} - \dfrac{\partial}{\partial \boldsymbol{\beta}} \log(1 + \exp(\boldsymbol{X_i \beta})) \right] \\
&= \sum_{i=1}^n \left[y_i \boldsymbol{X_i}^\top - \dfrac{\exp(\boldsymbol{X_i \beta})}{1 + \exp(\boldsymbol{X_i \beta})}\boldsymbol{X_i}^\top \right] \\
&= \sum_{i=1}^n \left[y_i -  \dfrac{\exp(\boldsymbol{X_i \beta})}{1 + \exp(\boldsymbol{X_i \beta})}\right]\boldsymbol{X_i}^\top
\end{align*}
and
\begin{align*}
\boldsymbol{g''(\beta)} \equiv \dfrac{\partial^2}{\partial\boldsymbol{\beta}}l(\boldsymbol{\beta};\boldsymbol{X}) &= \dfrac{\partial}{\partial\boldsymbol{\beta}}\sum_{i=1}^n \left[y_i -  \dfrac{\exp(\boldsymbol{X_i \beta})}{1 + \exp(\boldsymbol{X_i \beta})}\right]\boldsymbol{X_i}^\top \\
&= \sum_{i=1}^n - \boldsymbol X_i \boldsymbol X_i^\top \dfrac{\exp(\boldsymbol{X_i \beta})}{(1+\exp(\boldsymbol{X_i \beta}))^2}
\end{align*}
then, the iterative update is
$$\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^t - \boldsymbol{g''(\beta^{(t)})}^{-1}\boldsymbol{g'(\beta^{(t)})}$$
and we use an absolute convergence criteria so that we stop when
$$\left|\boldsymbol{\beta}^{(t+1)} - \boldsymbol{\beta}^t \right|_2 < \epsilon$$
```{r, echo = FALSE}
cancer_data <- rbind(c(1, 1, 0, 1), c(0, 1, 0, 1), c(1, 1, 2, 1), c(0, 1, 2, 1), 
                     c(1, 1, 4, 1), c(0, 1, 4, 1), c(1, 1, 5, 1), c(0, 1, 5, 1),
                     c(1, 1, 0, 0), c(0, 1, 0, 0), c(1, 1, 2, 0), c(0, 1, 2, 0), 
                     c(1, 1, 4, 0), c(0, 1, 4, 0), c(1, 1, 5, 0), c(0, 1, 5, 0)) %>%
                     as.matrix()
repTime <- c(9, 41 - 9, 94, 213 - 94, 53, 127 - 53, 60, 142 - 60,
             11, 67 - 11, 59, 211- 59, 53, 133 - 53, 28, 76 - 28)
cancer_data <- cancer_data[rep(1:nrow(cancer_data), times = repTime), ]
colnames(cancer_data) <- c('Y', 'intercept', 'gender', 'coffee')
Y <- cancer_data[,1]
X <- cancer_data[,c(2,3,4)]
```

```{r}
mod <- Newton_logit(b0=c(0,0,0),Y=Y,X=X,eps=1e-8)
```

Initialized at $(0,0,0)$, the optimizer converges after 5 iterations, yielding the estimates shown in the table below:

```{r, echo = FALSE}
Beta <- mod$Beta
I <- -mod$Hessian # fisher information
var <- solve(I) # variance
sig <- sqrt(diag(var)) # standard deviation
cbind(c("Beta0", "Beta1", "Beta2"), round(Beta,3), round(sig,3)) %>%
  data.frame() %>%
  kable(col.names = c(" ", "Coef. Est.", "Std. Error"),
        caption = "Results of Estimation")
```

### (c)

```{r, echo = FALSE, fig.align='center', out.width='60%'}
designX <- rbind(matrix(c(rep(1, 100), seq(0,5,length.out=100), rep(1,100)), ncol = 3, byrow = FALSE),
                 matrix(c(rep(1, 100), seq(0,5,length.out=100), rep(0,100)), ncol = 3, byrow = FALSE))

data.frame(designX[, -1], 
           p = exp(designX %*% Beta)/(1 + exp(designX %*% Beta))) %>%
  ggplot(aes(x = X1, y = p, color = factor(X2, labels = c("Female", "Male")))) +
  geom_line() +
  ylim(0, 0.75) +
  theme_classic() +
  labs(x = "Coffee Consumption", 
       y = "Predicted Prob. of Pancreatic Cancer",
       color = "Gender", 
       title = "Predicted Probability of Cancer") + 
  scale_color_manual(values=c("pink", "blue"))
```

The estimated log odds of getting pancreatic cancer for male is $$\log(\dfrac{\hat{p}}{1-\hat{p}}) = -0.791 + 0.138 * x_{coffee}$$ and the estimated log odds of getting pancreatic cancer for female is $$\log(\dfrac{\hat{p}}{1-\hat{p}}) = -1.188 + 0.138 * x_{coffee}$$
This means that, on average:

- while holding gender constant, one additional cup of coffee consumption would be associated with an increase of 0.138 in the **log** odds of getting pancreatic cancer, or an increase in the odds by a factor of $\exp(0.138) = 1.148$.

- while holding coffee consumption constant, males are associated with an increase of 0.397 in the **log** odds of getting pancreatic cancer as compared to females, or an increase in the odds by a factor of $\exp(0.397) = 1.488$.



### (d)

Using normal approximation (i.e. using a critical value of $z = 1.96$), testing against the null hypothesis that $H_0: \beta_i = 0$ for $i \in (0,1,2)$, we have strong enough evidence to conclude that all the coefficients are significantly different from zero as their z-scores are all larger than the critical value (in magnitude).

```{r}
I <- -mod$Hessian # fisher information
var <- solve(I) # variance
sig <- sqrt(diag(var)) # standard deviation
z <- c(mod$Beta) / sig # Z-scores
# abs(z) > 1.96 ---> TRUE, TRUE, TRUE
z
```