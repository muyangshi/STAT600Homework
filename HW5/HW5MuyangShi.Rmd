---
title: "HW5MuyangShi"
author: "Muyang Shi"
date: "2024-03-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', out.width = '60%')
library(Rcpp)
library(RcppArmadillo)
library(dplyr)
library(knitr)
library(gridExtra)
library(latex2exp)
library(ggplot2)
library(foreach)
library(doParallel)
library(coda)
library(nimble)
numcores <- detectCores()
registerDoParallel(numcores - 2)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
sourceCpp('MCMC.cpp')
```

Note: the cpp source code to this document can be found on my Github, listed as `MCMC.cpp`, [\textcolor{blue}{here}](https://github.com/muyangshi/STAT600Homework/tree/main/HW5).

## Problem

Assume the model
\begin{equation*}
X_j \sim 
  \begin{cases}
  \text{Poisson}(\lambda_1), & j = 1, ..., \theta \\
  \text{Poisson}(\lambda_2), & j = \theta+1, ..., 112
  \end{cases}
\end{equation*}
Assume $\lambda_i | \alpha \sim \text{Gamma}(3, \alpha)$ for $i = 1, 2$ where $\alpha \sim \text{Gamma(10, 10)}$ and assume $\theta$ follows a discrete uniform distribution over $\{1, ..., 111\}$. We would like to estimate the posterior distribution of the model parameters via a Gibbs sampler.

### (a) Derive the conditional distributions necessary to carry out Gibbs sampling for the change-point model.

The joint likelihood is:
\begin{align*}
p(\boldsymbol{\Theta} | \boldsymbol{X}) &\propto p(\boldsymbol{X} | \boldsymbol{\Theta})p(\boldsymbol{\Theta}) \\
&= p(\boldsymbol{X} | \lambda_1, \lambda_2, \theta) \cdot p(\lambda_1 | \alpha) p(\lambda_2 | \alpha) \cdot p(\alpha) p(\theta) \\
&= \prod_{j = 1}^{j = \theta} \dfrac{\lambda_1^{X_j}e^{-\lambda_1}}{X_j!} 
\prod_{j = \theta+1}^{j = 112} \dfrac{\lambda_2^{X_j}e^{-\lambda_2}}{X_j!} \cdot
\dfrac{\alpha^3}{\Gamma(3)}\lambda_1^{3-1}e^{-\alpha \lambda_1}
\dfrac{\alpha^3}{\Gamma(3)}\lambda_2^{3-1}e^{-\alpha \lambda_2} \cdot
\dfrac{10^{10}}{\Gamma(10)}\alpha^{10-1}e^{-10 \alpha}
\dfrac{1}{111} \mathbf{1}(\theta \in \{1, ..., 111\})
\end{align*}

And the full conditionals are:

For $\lambda_1$:
\begin{align*}
p(\lambda_1 | \boldsymbol{X}, \lambda_2, \alpha, \theta) & \propto p(\boldsymbol{X} | \lambda_1, \lambda_2, \alpha, \theta) p(\lambda_1 | \alpha) \\
&\propto \prod_{j=1}^{j=\theta} \dfrac{\lambda_1^{X_j}e^{-\lambda_1}}{X_j!} \lambda_1^{3-1}e^{-\alpha \lambda_1} \\
&\propto \lambda_1^{\sum_{j=1}^\theta X_j + 3 - 1}e^{-(\theta+\alpha)\lambda_1} \\
&\sim \text{Gamma}(3 + \sum_{j=1}^\theta X_j, \theta+\alpha)
\end{align*}
Similarly, for $\lambda_2$:
\begin{align*}
p(\lambda_2 | \boldsymbol{X}, \lambda_1, \alpha, \theta) &\propto \lambda_2^{\sum_{j=\theta+1}^{112} X_j + 3 - 1}e^{-(112 - \theta + \alpha) \lambda_2} \\
&\sim \text{Gamma}(3 + \sum_{j=\theta+1}^{112} X_j, 112 - \theta+\alpha)
\end{align*}
And for $\alpha$:
\begin{align*}
p(\alpha | \boldsymbol{X}, \lambda_1, \lambda_2, \theta) &\propto \alpha^3 e^{-\alpha \lambda_1} \alpha^3 e^{-\alpha \lambda_2} \alpha^{10-1} e^{-10\alpha} \\
&= \alpha^{16-1} e^{-(10 + \lambda_1 + \lambda_2) \alpha} \\
&\sim \text{Gamma}(16, 10 + \lambda_1 + \lambda_2)
\end{align*}
Finally for $\theta$:
\begin{align*}
p(\theta | \boldsymbol{X}, \lambda_1, \lambda_2, \alpha) &\propto \prod_{j = 1}^{j = \theta} \dfrac{\lambda_1^{X_j}e^{-\lambda_1}}{X_j!} 
\prod_{j = \theta+1}^{j = 112} \dfrac{\lambda_2^{X_j}e^{-\lambda_2}}{X_j!} \mathbf{1}(\theta \in \{1, ..., 111\}) \\
&\propto \lambda_1^{\sum_{j=1}^\theta X_j}e^{-\theta \lambda_1} \lambda_2^{\sum_{j=\theta+1}^{112} X_j}e^{-(112-\theta)\lambda_2}\mathbf{1}(\theta \in \{1, ..., 111\})
\end{align*}
Note that we can't see the distribution of $\theta$ through just eyeballing the equation. We'd be using a Metropolis approach -- 1.) proposing $\theta$ from the discrete Uniform between 1 and 111, 2.) calculate the likelihood (equation above) ratio to decide whether we accept or reject. (Also note that this proposal is symmetric, so there's no "Hasting" ratio.)

### (b) Implement the Gibbs sampler. Use a suite of convergence diagnostics to evaluate the convergence and mixing of your sampler.

The Gibbs sampler is implemented in `RcppArmadillo` with the function name `MCMC`. We ran four chains (for the Gelman diagnostic), and for inference we will use the first chain.

Note that, as we plot below, where 

- the traceplot looks fine in that the chain seems to have converged and the mixing is good, 
- the autocorrelation plot also seems fine in that we have decent effective sample size; $\theta$ is discrete so the traceplot for it actually doesn't look that bad
- and the Gelman diagnostic R is very close to 1 which is good (indicate that the chains have converged).

```{r}
coal <- read.table('coal.dat', skip = 1) # load data

# initial values
theta_0 <- 50
alpha_0 <- 1.1
lambda1_0 <- mean(coal[1:theta_0,2])
lambda2_0 <- mean(coal[theta_0:nrow(coal),2])

# run a couple chains using Rcpp
set.seed(2345)
chain1 <- MCMC(5000, coal$V2, lambda1_0, lambda2_0, alpha_0, theta_0)
chain2 <- MCMC(5000, coal$V2, lambda1_0, lambda2_0, alpha_0, theta_0)
chain3 <- MCMC(5000, coal$V2, lambda1_0, lambda2_0, alpha_0, theta_0)
chain4 <- MCMC(5000, coal$V2, lambda1_0, lambda2_0, alpha_0, theta_0)
```

```{r, echo = FALSE}
colnames(chain1) <- c('lambda1', 'lambda2', 'alpha', 'theta')
colnames(chain2) <- c('lambda1', 'lambda2', 'alpha', 'theta')
colnames(chain3) <- c('lambda1', 'lambda2', 'alpha', 'theta')
colnames(chain4) <- c('lambda1', 'lambda2', 'alpha', 'theta')
chain1.mcmc <- mcmc(chain1)
chain2.mcmc <- mcmc(chain2)
chain3.mcmc <- mcmc(chain3)
chain4.mcmc <- mcmc(chain4)

mcmc_list <- mcmc.list(list(chain1.mcmc, chain2.mcmc, chain3.mcmc, chain4.mcmc))
```

```{r}
plot(chain1.mcmc, density = FALSE)

autocorr.plot(chain1.mcmc)

gelman.diag(mcmc_list)
```

### (c) Construct density histograms and a table of summary statistics for the approximate posterior distributions of $\theta, \lambda_1$, and $\lambda_2$. Are symmetric HPD intervals appropriate for all of these parameters?

Here is the density histograms for the four parameters, $\lambda_1, \lambda_2, \alpha$ and $\theta$:

```{r, echo = FALSE}
plot(chain1.mcmc, trace = FALSE)
```

Here is a table of summary statistics for the approximate posterior distributions of $\theta, \lambda_1, \lambda_2$:

```{r, echo = FALSE}
summary(chain1.mcmc)
```

The skewness in the distributions aren't "severe" -- hence using symmetric HPD intervals are fine; however, one might argue that the distributions for $\lambda_1$, $\lambda_2$, as well as for $\theta$ are slightly right skewed, hence it might be better to use equal-tail intervals.

### (d) Interpret the results in the context of the problem.

Based on the posterior distributions from the MCMC, we believe there is a 95% probability that:

- $\lambda_1$, average number of yearly coal-mining disasters during the first period, is anywhere between 2.59 to 3.73
- $\lambda_2$, average number of yearly coal-mining disasters during the second period, is anywhere between 0.74 to 1.20
- $\theta$ is anywhere between 36 to 43, i.e. the transition happened anywhere between the year of 1886 to 1893.

### (e) Change the prior for $\lambda_1$ and $\lambda_2$ to a half-normal distribution $\propto N(0, \sigma^2)1_{[\lambda > 0]}$ with $\sigma^2$ known. Derive the appropriate algorithm (MH or Gibbs) to carry out inference on the posterior distribution of $\lambda_1$ and $\lambda_2$.

We begin with the joint likelihood:
\begin{align*}
p(\boldsymbol{\Theta} | \boldsymbol{X}) &\propto p(\boldsymbol{X} | \boldsymbol{\Theta})p(\boldsymbol{\Theta}) \\
&= p(\boldsymbol{X} | \lambda_1, \lambda_2, \theta) \cdot p(\lambda_1 | \sigma^2) p(\lambda_2 | \sigma^2) \cdot p(\theta) \\
&= \prod_{j = 1}^{j = \theta} \dfrac{\lambda_1^{X_j}e^{-\lambda_1}}{X_j!} 
\prod_{j = \theta+1}^{j = 112} \dfrac{\lambda_2^{X_j}e^{-\lambda_2}}{X_j!} \cdot
\dfrac{\sqrt{2}}{\sigma \sqrt{\pi}} \exp(-\dfrac{\lambda_1^2}{2\sigma^2})
\dfrac{\sqrt{2}}{\sigma \sqrt{\pi}} \exp(-\dfrac{\lambda_2^2}{2\sigma^2}) \cdot
\dfrac{1}{111} \mathbf{1}(\theta \in \{1, ..., 111\}) \\
&\propto \prod_{j = 1}^{j = \theta} \lambda_1^{X_j}e^{-\lambda_1}
\prod_{j = \theta+1}^{j = 112} \lambda_2^{X_j}e^{-\lambda_2} \cdot
\dfrac{1}{\sigma^2} \exp(-\dfrac{\lambda_1^2}{2\sigma^2}) \exp(-\dfrac{\lambda_2^2}{2\sigma^2}) \cdot
\mathbf{1}(\theta \in \{1, ..., 111\})
\end{align*}

And the full conditionals,

For $\lambda_1$:
\begin{align*}
p(\lambda_1 | \boldsymbol{X}, \lambda_2, \sigma, \theta) \propto \lambda_1^{\sum_{j=1}^\theta X_j} \exp(-\theta \lambda_1 - \dfrac{\lambda_1^2}{2\sigma^2})
\end{align*}

For $\lambda_2$:
\begin{align*}
p(\lambda_2 | \boldsymbol{X}, \lambda_1, \sigma, \theta) \propto \lambda_1^{\sum_{j=\theta+1}^{112} X_j} \exp(-(112-\theta) \lambda_1 - \dfrac{\lambda_2^2}{2\sigma^2})
\end{align*}

And for $\theta$:
\begin{align*}
p(\theta | \boldsymbol{X}, \lambda_1, \lambda_2, \sigma) 
&\propto \lambda_1^{\sum_{j=1}^\theta X_j}e^{-\theta \lambda_1} \lambda_2^{\sum_{j=\theta+1}^{112} X_j}e^{-(112-\theta)\lambda_2}\mathbf{1}(\theta \in \{1, ..., 111\})
\end{align*}

We will be using **random-walk metropolis** to update the parameters $\lambda_1$ and $\lambda_2$ (i.e. immediately reject if $\lambda_1$ and $\lambda_2$ falls out of support, so by doing it this way the hasting ratio will be 1).

### (f) Implement your MCMC algorithm from part (e). Try several values of $\sigma^2$ based on your understanding of the problem. (What is a reasonable variance for $\lambda$ based on what you know about the problem?).

The MH sampler is implemented in `RcppArmadillo` with the function name `MCMC_halfnorm`. We ran four chains (for the Gelman diagnostic) across three values of $\sigma \in \{0.5, 1.5, 2.5\}$, and for inference we will use the first chain for each $\sigma$.

```{r}
sigmas <- c(0.5,1.5,2.5)
set.seed(2345)

chain1.1 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[1], theta_0)
chain1.2 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[1], theta_0)
chain1.3 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[1], theta_0)
chain1.4 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[1], theta_0)

chain2.1 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[2], theta_0)
chain2.2 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[2], theta_0)
chain2.3 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[2], theta_0)
chain2.4 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[2], theta_0)

chain3.1 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[3], theta_0)
chain3.2 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[3], theta_0)
chain3.3 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[3], theta_0)
chain3.4 <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, sigmas[3], theta_0)
```

```{r, echo = FALSE}
colnames(chain1.1) <- c('lambda1', 'lambda2',  'theta')
colnames(chain1.2) <- c('lambda1', 'lambda2',  'theta')
colnames(chain1.3) <- c('lambda1', 'lambda2',  'theta')
colnames(chain1.4) <- c('lambda1', 'lambda2',  'theta')
chain1.1.mcmc <- mcmc(chain1.1)
chain1.2.mcmc <- mcmc(chain1.2)
chain1.3.mcmc <- mcmc(chain1.3)
chain1.4.mcmc <- mcmc(chain1.4)

colnames(chain2.1) <- c('lambda1', 'lambda2',  'theta')
colnames(chain2.2) <- c('lambda1', 'lambda2',  'theta')
colnames(chain2.3) <- c('lambda1', 'lambda2',  'theta')
colnames(chain2.4) <- c('lambda1', 'lambda2',  'theta')
chain2.1.mcmc <- mcmc(chain2.1)
chain2.2.mcmc <- mcmc(chain2.2)
chain2.3.mcmc <- mcmc(chain2.3)
chain2.4.mcmc <- mcmc(chain2.4)

colnames(chain3.1) <- c('lambda1', 'lambda2',  'theta')
colnames(chain3.2) <- c('lambda1', 'lambda2',  'theta')
colnames(chain3.3) <- c('lambda1', 'lambda2',  'theta')
colnames(chain3.4) <- c('lambda1', 'lambda2',  'theta')
chain3.1.mcmc <- mcmc(chain3.1)
chain3.2.mcmc <- mcmc(chain3.2)
chain3.3.mcmc <- mcmc(chain3.3)
chain3.4.mcmc <- mcmc(chain3.4)

mcmc_list.1 <- mcmc.list(list(chain1.1.mcmc, chain1.2.mcmc, chain1.3.mcmc, chain1.4.mcmc))
mcmc_list.2 <- mcmc.list(list(chain2.1.mcmc, chain2.2.mcmc, chain2.3.mcmc, chain2.4.mcmc))
mcmc_list.3 <- mcmc.list(list(chain3.1.mcmc, chain3.2.mcmc, chain3.3.mcmc, chain3.4.mcmc))
```

We looked at the traceplot, autocorrelation plots, and the Gelman potential scale reduction factors, and decide that the model with $\sigma = 1.5$ yields the best result. Below are the traceplots, density plots, autocorrelation plots, and the Gelman PSRF for the model with $\sigma = 1.5$:

```{r, echo = FALSE, out.width='75%'}
# plot(chain1.1.mcmc, density = FALSE)
# autocorr.plot(chain1.1.mcmc)
# gelman.diag(mcmc_list.1)

plot(chain2.1.mcmc, density = TRUE)
autocorr.plot(chain2.1.mcmc)
gelman.diag(mcmc_list.2)

# plot(chain3.1.mcmc, density = FALSE)
# autocorr.plot(chain3.1.mcmc)
# gelman.diag(mcmc_list.3)
```

### (g) Provide a few key results to compare the inferences that you'd make based on the two models. Which model do you prefer and why?

Here is a summary of posterior for the halfnorm model with $\sigma = 1.5$:
```{r, echo = FALSE}
summary(chain2.1.mcmc)
```
Based on the posterior distributions from the MCMC, we believe there is a 95% probability that:

- $\lambda_1$, average number of yearly coal-mining disasters during the first period, is anywhere between 2.60 to 3.63
- $\lambda_2$, average number of yearly coal-mining disasters during the second period, is anywhere between 0.72 to 1.16
- $\theta$ is anywhere between 36 to 46, i.e. the transition happened anywhere between the year of 1886 to 1896.

Note that qualitatively, the results is similar to the Gamma-Poisson model -- estimates for $\lambda_1$ being larger than $\lambda_2$ suggests that the reduction in yearly coal-mining disasters; the estimates for $\theta$ (change point) also being similar between the two models.

However, I would prefer the Gamma-Poisson model, because:

- Gibbs sampling does not involve the Metropolis-Hasting step, so mixing would not be an issue as we have the closed form posterior.

- Due to not having the Metropolis-Hasting update, the Gibbs sampler is also marginally faster than the MH.

### (h) Run both models in `Nimble`. Compare the results and speed from your code and Nimble's. Briefly discuss.

Below we implement the model in `nimble`. Note that `nimble` by default treats all variables as continuous, so we use the step function for $\theta$ to make it discrete uniform in $\{1, ..., 111\}$ when deciding between $\lambda_1$ and $\lambda_2$ for the Poisson, and for the traceplot we use the `floor` function on $\theta$.

```{r, results = 'hide', warning = FALSE, message = FALSE}
coalCode <- nimbleCode({
  alpha ~ dgamma(10, 10)
  lambda1 ~ dgamma(3, alpha)
  lambda2 ~ dgamma(3, alpha)
  theta ~ dunif(1, 112)
  
  for (i in 1:112){
    X[i] ~ dpois(lambda1 * step(theta - i) + lambda2 * (1-step(theta-i)))
  }
})
coalData <- list(X = coal[,2])
coalInits <- list(alpha = 1.1,
                  theta = 50,
                  lambda1 = 2.7,
                  lambda2 = 0.89)
coalModel <- nimbleModel(coalCode, 
                      data = coalData, 
                      inits = coalInits) # pump
CcoalModel <- compileNimble(coalModel)
coalConf <- configureMCMC(coalModel)
coalConf$addMonitors(c("lambda1","lambda2","alpha","theta"))
coalMCMC <- buildMCMC(coalConf)
CcoalMCMC <- compileNimble(coalMCMC, project = coalModel)
coalSamples.1 <- runMCMC(CcoalMCMC, niter = 5000)
coalSamples.2 <- runMCMC(CcoalMCMC, niter = 5000)
coalSamples.3 <- runMCMC(CcoalMCMC, niter = 5000)
coalSamples.4 <- runMCMC(CcoalMCMC, niter = 5000)
```

Below are traceplots, density plots, auto-correlation plots, and Gelman diagnostics for the chain run by `nimble` -- the chains have converged.

```{r, echo = FALSE}
coalSamples.1[,4] <- floor(coalSamples.1[,4])
coalSamples.2[,4] <- floor(coalSamples.2[,4])
coalSamples.3[,4] <- floor(coalSamples.3[,4])
coalSamples.4[,4] <- floor(coalSamples.4[,4])

chain1.nimble <- mcmc(coalSamples.1)
chain2.nimble <- mcmc(coalSamples.2)
chain3.nimble <- mcmc(coalSamples.3)
chain4.nimble <- mcmc(coalSamples.4)

mcmc_list.nimble <- mcmc.list(list(chain1.nimble, chain2.nimble,chain3.nimble,chain4.nimble))

plot(chain1.nimble, density = FALSE)
plot(chain1.nimble, trace = FALSE)
autocorr.plot(chain1.nimble)
gelman.diag(mcmc_list.nimble)
```

Based on the summary of the posterior distributions from the `nimble` MCMC below, we believe there is a 95% probability that:

- $\lambda_1$, average number of yearly coal-mining disasters during the first period, is anywhere between 2.55 to 3.69
- $\lambda_2$, average number of yearly coal-mining disasters during the second period, is anywhere between 0.73 to 1.18
- $\theta$ is anywhere between 36 to 46, i.e. the transition happened anywhere between the year of 1886 to 1896.

Note that again qualitatively, the results is very similar to the models we implemented by ourselves -- estimates for $\lambda_1$ being larger than $\lambda_2$ suggests that the reduction in yearly coal-mining disasters; the estimates for $\theta$ (change point) also being similar between the two models:

```{r, echo = FALSE}
summary(chain1.nimble)
```

Speed-wise, note that **our models** are of 2 orders of magnitude faster than the `nimble` model.

```{r, message=FALSE, warning=FALSE}
start.time <- Sys.time()
a <- runMCMC(CcoalMCMC, niter = 5000, progressBar = FALSE)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

start.time <- Sys.time()
b <- MCMC(5000, coal$V2, lambda1_0, lambda2_0, alpha_0, theta_0)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

start.time <- Sys.time()
c <- MCMC_halfnorm(5000, coal$V2, lambda1_0, lambda2_0, 1.5, theta_0)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
```